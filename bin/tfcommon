#!/usr/bin/env bash

## INIT for all uses

# check existence of terraform binary
if ! type terraform >/dev/null 2>&1; then
  echo "terraform not found in PATH"
  exit 1
fi

terraform --version

# use local folder for backend, override ~/.tfbackend with env BACKEND_PATH
backend_path=${BACKEND_PATH:-~/.tfbackend}
if [ ! -d "$backend_path" ]; then
  mkdir -p "$backend_path"
fi

export TF_CLI_ARGS_init="-compact-warnings"

## Functions
is_tf_folder() {
  local folder="$1"

  if [[ ! -d "$folder" ]]; then
    echo "Error: '$folder' is not a valid directory."
    return 1
  fi

  status=1
  find "$folder" -maxdepth 1 -type f -name "*.tf" -print -quit 2>/dev/null | grep . >/dev/null

  if [[ $? -eq 0 ]]; then
    return 0
  else
    echo "No files with extension 'tf' found in '$folder'."
    return 1
  fi
}

# setupbackend <project_name>
#     current directory will be "folder" for this action
setupbackend() {
  project_name=$1
  folder=$(basename $PWD)

  terraform_bucket=${TERRAFORM_S3_BUCKET:-cookson-pro-tf-backend}
  # For CI/CD, use an assumable role, assume circleci role for now
  # make profile dynamic
  cat <<EOF >work/backend.tf
  terraform {
    backend "s3" {
      bucket = "${terraform_bucket}"
      key    = "${project_name}/${folder}"
      region = "us-east-1"
      profile = "s3-backend"
    }
  }
EOF
  echo "S3 Backend Set Up: ${project_name}/${folder}"
  cat work/backend.tf
}

detectnpmbuild() {
  # Check if package.json exists in the current directory
  if [[ -f "package.json" ]]; then
    build_script=$(npm pkg get scripts.build_lambda 2>/dev/null)

    if [[ "$build_script" != "{}" ]] && [[ -n "$build_script" ]]; then
      echo "Build script found in package.json: $build_script"
      return 0
    else
      # echo "No 'build' script found in package.json."
      return 1
    fi

    return 0
  else
    # echo "Not an npm project"
    return 1
  fi
}

installlambdas() {
  # Clone lambda repositories if LAMBDA_REPOS is set
  # "npm run build_lambda" must produce a lambda.zip in the repo's folder
  if [[ -n "$LAMBDA_REPOS" ]]; then
    echo "Cloning and building lambda repos"
    # Split LAMBDA_REPOS by comma and iterate over each repo
    IFS=',' read -ra repos <<<"$LAMBDA_REPOS"
    for repo in "${repos[@]}"; do
      echo "Cloning lambda repo: $repo"
      git clone "$repo"
      # run npm instal and build for each repo
      repo_name=$(basename "$repo" .git)
      if [[ -d "$repo_name" ]]; then
        pushd "$repo_name"
        if [[ detectnpmbuild -eq 0 ]]; then
          echo "Detected npm project in $repo_name, running npm install and build"
          npm install && npm run build_lambda
        else
          echo "Skipping lambda install"
        fi
        popd # go back to the work folder
      fi
    done
  fi
}

loadjobvars() {
  # Load job variables from the environment or a file
  if [[ -f "job.vars" ]]; then
    echo "Loading job variables from job.vars"
    source job.vars
  elif [[ -f "job.vars" ]]; then
    echo "Loading job variables from job.vars"
    source job.vars
  else
    echo "No job.vars file found, using environment variables"
  fi

  # Check if LAMBDA_REPOS is set, if not, set it to an empty string
  LAMBDA_REPOS=${LAMBDA_REPOS:-""}
}

# setupworkfolder <folder>
#     currend directory has to be main project root
setupworkfolder() {
  set -x
  local folder="$1"

  echo "Setting up $folder"
  if [[ ! -d "$folder/work" ]]; then
    echo "Creating work folder for $folder"
    mkdir -p "$folder/work"
  fi
  # TODO: make this circle-ci agnostic
  local project_name=${CIRCLE_PROJECT_REPONAME:-$(basename $PWD)}

  echo "Project Name: $project_name"
  cd "$folder"

  echo "Removing old work files"
  rm -fr work/* # wipe all work files

  loadjobvars

  setupbackend "$project_name"

  # Find directories directly under the current path (.), at depth 1.
  # -type d: only directories.
  # ! -name '*.*': directory name does not contain a dot. This excludes hidden folders (e.g. .git) and folders with dots (e.g. my.folder).
  # ! -name 'work': exclude the 'work' directory itself.
  # -print: prints the found path.
  # Errors (e.g., permission denied) are suppressed.
  local found_folders
  found_folders=$(find . -maxdepth 1 -type d ! -name '*.*' ! -name 'work' -print 2>/dev/null)

  [[ ! -z "$found_folders" ]] && echo "Folders to symlink into 'work': $found_folders"
  # symlink any non-dot folders to work, mainly for static or auxilliary files
  for folder_path in $found_folders; do
    local dir_name
    dir_name=$(basename "$folder_path") # Extracts 'gcp' from './gcp'

    if [[ -n "$dir_name" ]]; then # Ensure dir_name is not empty
      echo "Symlinking ../$dir_name to work/$dir_name"
      # Create a relative symlink inside the 'work' directory.
      # ln -sfn TARGET LINK_NAME
      # TARGET is ../$dir_name (relative to the 'work' directory)
      # LINK_NAME is work/$dir_name
      # -f: force (overwrite if exists)
      # -n: no-dereference (if work/$dir_name is a symlink to a dir, replace the symlink itself)
      ln -sfn "../$dir_name" "work/$dir_name"
    fi
  done

  [[ -n "$INCLUDE_ZIPS" ]] && cp *.zip work
  cp *.tf terraform.tfvars work
  cd work

  installlambdas

  getcreds
}

getawscreds() {
  local aws_found=0
  grep aws provider.tf 2>/dev/null >/dev/null
  if [[ $? == 0 ]]; then
    aws_found=1
  fi
  grep s3 backend.tf 2>/dev/null >/dev/null
  if [[ $? == 0 ]]; then
    aws_found=1
  fi

  if [[ ${aws_found} -eq 0 ]]; then
    return 0
  fi

  echo "Detected AWS provider"

  which op 2>/dev/null >/dev/null
  # if onepassword cli installed (local dev) retrieve key ID and secret, will be used to assume oidc-profile
  if [[ $? -eq 0 ]]; then
    export AWS_ACCESS_KEY_ID=$(op item get 'Terraform AWS Key' --fields 'access key id')
    export AWS_SECRET_ACCESS_KEY=$(op item get 'Terraform AWS Key' --reveal --fields 'secret access key')
  fi

  # NOTE: locally and on circleci, backend uses s3-backend profile explicitly, and terraform plan/apply uses oidc-profile explicitly
  # echo "using terraform's oidc-profile to use an OIDC session already created"
  # export AWS_PROFILE=oidc-profile
}

getazurecreds() {
  # echo "Checking AZURE creds $(grep azurerm provider.tf)"
  grep azurerm provider.tf 2>/dev/null >/dev/null
  if [[ ! $? -eq 0 ]]; then
    return 0
  fi

  echo "Detected Azure template"

  authjson=$(az account list 2>/dev/null)
  export ARM_SUBSCRIPTION_ID=$(echo "$authjson" | jq -r .[0].id)
  export ARM_TENANT_ID=$(echo "$authjson" | jq -r .[0].tenantId)

  echo "Tenant ID $ARM_TENANT_ID"
  echo "Subscription ID $ARM_SUBSCRIPTION_ID"
}

# GCP depends on the CLI gcloud being logged in, so that's a pre-requisite for now
getcreds() {
  getawscreds
  getazurecreds
}
